<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Proabilistic Matrix Factorization | Shivansh Gupta </title> <meta name="author" content="Shivansh Gupta"> <meta name="description" content="Scalable solution for large, sparse, and imbalanced data"> <meta name="keywords" content="academic-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://gshivansh2001.github.io/blog/2024/pmf/"> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Shivansh Gupta </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Research </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/books/">Beyond Research </a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Proabilistic Matrix Factorization</h1> <p class="post-meta"> Created on May 02, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>This paper<sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup> presents the Probabilistic Matrix Factorization (PMF) model which scales linearly with the number of observations and performs well on the large, sparse, and imbalanced dataset. Before going into the detail of this model, let’s discuss why it is even needed?</p> <p>Recommendation systems enhance user experiences by suggesting relevant items based on preferences. They primarily use two traditional methods: collaborative filtering, which recommends items liked by other users with similar tastes and content-based filtering, which recommends items similar to those a user has liked before. The most popular approach to collaborative filtering is based on low-dimensional factor models.</p> <p>Many of these low-dimensional models are probabilistic factor-based models. The majority of these probabilistic factor-based models attempt to explain observations through underlying factors or variables (hidden factors; these represent latent characteristics). The problem with these models is that exact inference is intractable, meaning that determining the distribution of hidden factors given the observed data is computationally impractical. This necessitates the use of approximations to estimate the posterior distribution of hidden factors based on observed data. Singular Value Decomposition (SVD) is one of the low-rank approximation algorithms based on minimizing the sum-squared distance. It finds the matrix \(\hat R=U^TV\) of the given rank which minimizes the sum-squared distance to the target matrix \(R\). Collaborative filtering algorithms need to scale linearly with number of observations and perform well on very sparse and imbalanced datasets. Most of the aforementioned methods prove infeasible for various reasons. Firstly, except for the matrix-factorization based methods, none scale well to large datasets. Secondly, if most of the rating entries are missing, it leads to inaccurate predictions. Here, PMF comes into the picture as it solves both problems: it scales linearly with the number of observations and performs well on very sparse and imbalanced datasets.</p> <h3 id="pmf-algorithm">PMF algorithm</h3> <p>Suppose we have \(M\) movies, \(N\) users, and \(R_{ij}\) represent the rating of user \(i\) for movie \(j\). (for the sake of simplicity it’s assumed rating values are integer \(\in [1, K]\))<br></p> <div align="center"> $$U \in R^{D\times N}$$ (latent user matrix)<br> $$V \in R^{DxM}$$ (latent movie matrix) </div> <p><br> For test set, assume a probabilistic linear model (relationship between input and output is linear but observed outputs deviate from this linear relationship due to random noise). Define conditional distribution over the observed ratings as: <br></p> <div align="center"> $$ p(R|U, V, \sigma ^2)=\prod_{i = 1}^{N}\prod_{j = 1}^{M}\left[ N(R_{ij}|U^T_iV_j, \sigma ^2)\right]^{I_{ij}} $$ </div> <p>where, <br> \(\hspace{20pt} N(x|\mu, \sigma ^2)\) denotes probability density function of Gaussian distribution with mean \(\mu\) and variance \(\sigma ^2\) <br> \(\hspace{20pt}I_{ij}=1\), if user \(i\) rated movie \(j\)<br> \(\hspace{33pt}=0\), otherwise <br> <br> We also place zero-mean spherical Gussian priors on user and movie feature vectors. <br> <br> \(\hspace{60pt} p(U|\sigma ^2_{U})=\prod_{i=1}^{N} N(U_i|0, \sigma ^2_{U}I), \quad p(V|\sigma ^2_{V})=\prod_{i=1}^{N} N(V_i|0, \sigma ^2_{V}I)\) <br> <br> Using Bayes’ Rule, we find that the posterior distribution over latent vectors \(U_i\) and \(V_i\) given the observed ratings \(R\) is proportional to the product of the likelihood of the observed data and the prior distribution. Taking log of the posterior distribution over the user and movie features, we get:<br> <br> \(\hspace{10pt} ln \, p(U, V|R, \sigma ^2, \sigma ^2_V, \sigma ^2_U) = - \frac{1}{2\sigma ^2}\sum_{i = 1}^{N}\sum_{j = 1}^{M}I_{ij}(R_{ij}-U_i^TV_j)^2 - \frac{1}{2\sigma ^2_U}\sum_{i = 1}^{N}U_i^TU_i - \frac{1}{2\sigma ^2_V}\sum_{j = 1}^{M}V_j^TV_j\) \(\hspace{125pt} - \frac{1}{2}\left( \left( \sum_{i = 1}^{N}\sum_{j = 1}^{M}I_{ij}\right )ln\, \sigma ^2 + ND ln\, \sigma ^2_U + MD ln \,\sigma ^2_V \right)\) <br> <br> Maximising the log-posterior over movie and user features with hyperparameters (observation noise variance \(\sigma\) and prior variances \(\sigma_U, \sigma_V\)) kept fixed is equivalent to minimizing the sum-of-squared-errors obective function with quadratic regularization terms: <br> <br> \(\hspace{40pt} E = \frac{1}{2}\sum_{i = 1}^{N}\sum_{j = 1}^{M}I_{ij}(R_{ij}-U_i^TV_j)^2 + \frac{\lambda _U}{2}\sum_{i = 1}^{N} ||U_i||^2_{Fro} + \frac{\lambda _V}{2}\sum_{j = 1}^{M} ||V_i||^2_{Fro}\) <br> <br> \(\hspace{150pt}\) where, \(\lambda _U=\frac{\sigma ^2}{\sigma ^2_U}, \lambda _V=\frac{\sigma ^2}{\sigma ^2_V}\) <br> <br> A local minimum of this objective function can be found by performing gradient descent in \(U\) and \(V\). <br></p> <blockquote> <p>If all the ratings have been observed it means the estimates of the parameters (eg. user and latent features) can be made with higher confidence and potentially less variance in the estimates themselves.</p> </blockquote> <p>PMF model extends traditional SVD by incorporating probabilistic elements (gaussian noise in observed ratings, gaussian priors for latent features). This probabilistic framework accounts for uncertainty in observation (ratings) and parameter estimates (latent features) offering a more flexible and potentially robust approach compared to deterministic SVD.</p> <p>In a typical real-world scenario, not all user-item ratings are available. However, if hypothetically, all ratings were observed, the PMF model would still need to balance fitting the observed data (through the likelihood term) and maintaining reasonable complexity (through the prior regularization terms).</p> <p>If prior variances are made very large, then it effectively weakens the influence of the prior distribution, making it non-informative. This means the latent features are less regularized and the model focuses more on fitting the observed data. In the limit, when the prior variances go to infinity, the regularization effect vanishes, and the PMF model then essentially reduces to performing an SVD.</p> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1"> <p>R. Salakhutdino and A. Mnihv, Probabilistic Matrix Factorization <a href="#fnref:1" class="reversefootnote" role="doc-backlink">↩</a></p> </li> </ol> </div> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/sdidI/">What Really Happens When You Add Covariates to SDID in Stata</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/dp/">How SDID Works When Covariates Are Used in Stata</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/vc/">Variational Calculus</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/rdp/">Rényi DP</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Shivansh Gupta. Last updated: December 06, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> </body> </html>