<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://gshivansh2001.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://gshivansh2001.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-12-25T07:49:25+00:00</updated><id>https://gshivansh2001.github.io/feed.xml</id><title type="html">Shivansh Gupta</title><subtitle>Shivansh Gupta&apos;s Academic Webpage</subtitle><entry><title type="html">What Really Happens When You Add Covariates to SDID</title><link href="https://gshivansh2001.github.io/blog/2025/sdidI/" rel="alternate" type="text/html" title="What Really Happens When You Add Covariates to SDID"/><published>2025-02-14T00:00:00+00:00</published><updated>2025-02-14T00:00:00+00:00</updated><id>https://gshivansh2001.github.io/blog/2025/sdidI</id><content type="html" xml:base="https://gshivansh2001.github.io/blog/2025/sdidI/"><![CDATA[<p>A few days ago, while working on an analysis for a project, I found myself stuck on a deceptively simple question: <strong>what exactly happens when we add covariates to a Synthetic Difference-in-Differences (SDID) model in Stata?</strong> I had used SDID before, I had added covariates before, but this was the first time I <em>actually</em> needed to understand how SDID treated them beneath the hood. The more I looked into it, the clearer it became that SDID handles covariates in a way that is <em>very</em> different from OLS or fixed-effects regressions.</p> <p>So I spent a couple of days reading the literature, digging through the Stata help files, and scrolling through Statalist threads where researchers faced the same questions. This post is a summary of that journey — written the way I wish I had found it when I first went searching.</p> <hr/> <h4 id="why-this-problem-caught-my-attention">Why This Problem Caught My Attention</h4> <p>One reason I found this problem particularly interesting is that covariates in SDID behave very differently from how we use controls in standard regression frameworks like <code class="language-plaintext highlighter-rouge">reghdfe</code> or <code class="language-plaintext highlighter-rouge">xtreg</code>. In a typical regression, covariates enter the model as explanatory variables whose coefficients we interpret, test, and often use to reason about underlying mechanisms. The regression framework is built around the idea that these coefficients have a meaning of their own.</p> <p>In SDID, however, covariates serve an entirely different purpose. They are not part of the causal narrative, and their coefficients are not designed to be interpreted. Instead, covariates act as tools for <em>cleaning</em> the outcome variable. SDID uses them to strip away predictable variation so that the remaining residual trends are more comparable between treated and control units. The goal is not to estimate how covariates affect the outcome, but to improve the quality of the synthetic counterfactual.</p> <p>Realizing this difference was helpful because it forced me to rethink what covariates <em>do</em> in SDID. They are not contributors to the final ATT estimate; they are part of a preprocessing step that shapes the data SDID uses. Once I internalized that SDID treats covariates as adjustment mechanisms rather than explanatory variables, the rest of the method — and the behavior of <code class="language-plaintext highlighter-rouge">optimized</code> vs. <code class="language-plaintext highlighter-rouge">projected</code> — became much clearer.</p> <hr/> <p>When we add covariates to <strong>Synthetic Difference-in-Differences (SDID)</strong>, the intention is not to estimate the causal effect of those covariates. In fact, SDID does not even try to report meaningful covariate coefficients. Instead, covariates are used to <strong>improve the construction of the synthetic control</strong>. They help clean the outcome by explaining predictable variation so that the SDID weights — the unit weights $\omega$ and the time weights $\lambda$ — are chosen using outcomes that have been “purified” of covariate-driven noise.</p> <p>This adjustment happens before the SDID optimization step. And depending on how we residualize the outcome, the process can behave quite differently.</p> <hr/> <h4 id="the-residualized-approach">The Residualized Approach</h4> <p>My first stop was the standard, “optimized” method in Stata. The logic behind it is straightforward. Before SDID starts balancing treated and control units, it <em>first</em> regresses the outcome on the covariates:</p> \[\tilde Y_{it} = Y_{it} - X_{it}\hat\beta\] <p>and then uses $\tilde Y_{it}$ — not the raw outcomes — to compute the synthetic weights. This is reminiscent of partialling out covariates before matching. In practice, the procedure works well when covariates behave similarly for treated and control units, but can struggle when they differ sharply. Because the regression that obtains $\hat\beta$ uses <em>all</em> units (treated and untreated), any imbalance between the two groups flows into the residualization step. As a result, this version of SDID often produces covariate coefficients that cannot be replicated by any standard regression.</p> <hr/> <h4 id="the-projected-approach">The Projected Approach</h4> <p>Then I discovered the “projected” method — a quieter option hidden in the Stata help file but discussed extensively on Statalist. This method computes $\hat\beta$ using <strong>only the untreated units</strong>. The idea is elegant: if covariates are supposed to explain untreated outcome variation, then only untreated units should inform their relationship with the outcome. Once these coefficients are estimated, residuals are computed and SDID proceeds normally.</p> <p>What surprised me is how different the results can be. In my own replications — and in several Statalist posts — the projected method produced covariate coefficients that aligned perfectly with a simple fixed-effects regression on the untreated sample. The optimized method did not.</p> <p>At that point, the difference made sense: the two methods are solving two different problems. One residualizes using information from <em>everyone</em>; the other uses only <em>controls</em>, preserving the logic of a synthetic control.</p> <hr/> <h4 id="a-note-on-joint-estimation-sdidc">A Note on Joint Estimation (SDIDC)</h4> <p>During this process, I also came across a newer method — SDIDC by Hirshberg and Klosin — which estimates the covariate coefficients and the SDID weights jointly rather than separately. This approach avoids some of the pitfalls of residualizing first and weighting later. It’s still new, but it’s a promising direction, especially when covariates strongly overlap with latent trends.</p> <hr/> <h4 id="what-actually-changes-when-covariates-enter-the-model">What Actually Changes When Covariates Enter the Model</h4> <p>One of the most important insights I gained is that <strong>covariates do not change the structure of the SDID estimator itself</strong>. The minimization problem is the same. What changes are the data fed into it. Once we residualize the outcome, SDID computes weights based on $\tilde Y_{it}$, so the resulting $\omega$ and $\lambda$ often look different from the no-covariate case. Consequently, the ATT estimate also shifts.</p> <p>Stata stores the covariate coefficients it used inside <code class="language-plaintext highlighter-rouge">e(beta)</code>, but these are not causal effects and should not be interpreted as such. In fact, most covariates end up statistically insignificant — even when the ATT remains highly significant — because significance was never the goal. The covariates are there to improve balance, not to explain outcomes in a causal sense.</p> <hr/> <h4 id="using-covariates-in-stata-what-i-learned">Using Covariates in Stata: What I Learned</h4> <p>I went back to Stata and experimented with the two approaches. The commands looked something like this:</p> <div class="language-stata highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sdid</span><span class="w"> </span><span class="n">outcome</span><span class="w"> </span><span class="n">unitid</span><span class="w"> </span><span class="n">time</span><span class="w"> </span><span class="n">treated</span><span class="p">,</span><span class="w"> </span><span class="n">covariates</span><span class="p">(</span><span class="n">age</span><span class="w"> </span><span class="n">income</span><span class="w"> </span><span class="n">x1</span><span class="w"> </span><span class="n">x2</span><span class="p">,</span><span class="w"> </span><span class="n">optimized</span><span class="p">)</span>
</code></pre></div></div> <p>and</p> <div class="language-stata highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sdid</span><span class="w"> </span><span class="n">outcome</span><span class="w"> </span><span class="n">unitid</span><span class="w"> </span><span class="n">time</span><span class="w"> </span><span class="n">treated</span><span class="p">,</span><span class="w"> </span><span class="n">covariates</span><span class="p">(</span><span class="n">age</span><span class="w"> </span><span class="n">income</span><span class="w"> </span><span class="n">x1</span><span class="w"> </span><span class="n">x2</span><span class="p">,</span><span class="w"> </span><span class="n">projected</span><span class="p">)</span>
</code></pre></div></div> <p>The ATT estimates were reasonably close, but the covariate coefficients were completely different — and only the projected version matched the coefficients from:</p> <div class="language-stata highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">egen</span><span class="w"> </span><span class="n">W</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">mean</span><span class="p">(</span><span class="n">treated</span><span class="p">),</span><span class="w"> </span><span class="n">by</span><span class="p">(</span><span class="n">unitid</span><span class="p">)</span>

<span class="n">reghdfe</span><span class="w"> </span><span class="n">outcome</span><span class="w"> </span><span class="n">age</span><span class="w"> </span><span class="n">income</span><span class="w"> </span><span class="n">x1</span><span class="w"> </span><span class="n">x2</span><span class="w"> </span><span class="kr">if</span><span class="w"> </span><span class="n">W</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="n">absorb</span><span class="p">(</span><span class="n">unitid</span><span class="w"> </span><span class="n">time</span><span class="p">)</span><span class="w"> </span><span class="n">cluster</span><span class="p">(</span><span class="n">unitid</span><span class="p">)</span>
</code></pre></div></div> <p>Once I understood the logic, the mismatch stopped bothering me. Optimized and projected are not competing versions of the same method; they are solving different residualization problems.</p> <hr/> <h4 id="do-covariates-actually-help">Do Covariates Actually Help?</h4> <p>In my own analysis, adding covariates helped reduce noise in pre-treatment trends for the treated units. But it’s not guaranteed. The only reliable way to check is to look at:</p> <ol> <li><strong>Pre-treatment fit</strong> — do the synthetic and treated paths align more closely?</li> <li><strong>Weight stability</strong> — do unit weights become more reasonable?</li> <li><strong>Robustness across residualization methods</strong> — if optimized and projected produce wildly different ATTs, it may signal an identification issue.</li> <li><strong>Warnings from the optimization</strong> — SDID sometimes struggles to find weights when covariates introduce multicollinearity.</li> </ol> <p>Like everything else in applied econometrics, covariates help when they help — and sometimes they don’t.</p> <hr/> <h4 id="a-small-conceptual-example">A Small Conceptual Example</h4> <p>To build intuition, imagine studying how a policy affects student test scores. Covariates like <code class="language-plaintext highlighter-rouge">income</code> or <code class="language-plaintext highlighter-rouge">school_resources</code> explain part of the variation in scores that has nothing to do with the treatment. By removing this predictable variation first, SDID focuses only on aligning the remaining, unexplained trends across treated and control units. If treated schools differ systematically in those covariates, the projected method tends to produce more stable residuals, which leads to more reliable weights.</p> <hr/> <h4 id="further-reading">Further Reading</h4> <p>A few resources I found especially helpful:</p> <ul> <li><a href="https://www.aeaweb.org/articles?id=10.1257/aer.20190159">Arkhangelsky et al. (2021), <em>Synthetic Difference-in-Differences</em></a></li> <li>Stata’s official documentation for <code class="language-plaintext highlighter-rouge">sdid</code></li> <li><a href="https://klosins.github.io/Hirshberg_Klosin_SDIDC.pdf">Hirshberg &amp; Klosin (2024), <em>Synthetic Differences-in-Differences with Covariates</em></a></li> </ul> <hr/> <p>If you’re working on SDID analyses of your own, especially with covariates, I hope this saves you a few hours of searching. For me, the trick was simply realizing that covariates in SDID aren’t part of the causal story — they’re part of the <em>cleaning</em> story. Once that clicked, everything else became much clearer.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[An early researcher’s walkthrough on understanding covariates in synthetic difference-in-differences]]></summary></entry><entry><title type="html">Proabilistic Matrix Factorization</title><link href="https://gshivansh2001.github.io/blog/2024/pmf/" rel="alternate" type="text/html" title="Proabilistic Matrix Factorization"/><published>2024-05-02T00:00:00+00:00</published><updated>2024-05-02T00:00:00+00:00</updated><id>https://gshivansh2001.github.io/blog/2024/pmf</id><content type="html" xml:base="https://gshivansh2001.github.io/blog/2024/pmf/"><![CDATA[<p>This paper<sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup> presents the Probabilistic Matrix Factorization (PMF) model which scales linearly with the number of observations and performs well on the large, sparse, and imbalanced dataset. Before going into the detail of this model, let’s discuss why it is even needed?</p> <p>Recommendation systems enhance user experiences by suggesting relevant items based on preferences. They primarily use two traditional methods: collaborative filtering, which recommends items liked by other users with similar tastes and content-based filtering, which recommends items similar to those a user has liked before. The most popular approach to collaborative filtering is based on low-dimensional factor models.</p> <p>Many of these low-dimensional models are probabilistic factor-based models. The majority of these probabilistic factor-based models attempt to explain observations through underlying factors or variables (hidden factors; these represent latent characteristics). The problem with these models is that exact inference is intractable, meaning that determining the distribution of hidden factors given the observed data is computationally impractical. This necessitates the use of approximations to estimate the posterior distribution of hidden factors based on observed data. Singular Value Decomposition (SVD) is one of the low-rank approximation algorithms based on minimizing the sum-squared distance. It finds the matrix \(\hat R=U^TV\) of the given rank which minimizes the sum-squared distance to the target matrix \(R\). Collaborative filtering algorithms need to scale linearly with number of observations and perform well on very sparse and imbalanced datasets. Most of the aforementioned methods prove infeasible for various reasons. Firstly, except for the matrix-factorization based methods, none scale well to large datasets. Secondly, if most of the rating entries are missing, it leads to inaccurate predictions. Here, PMF comes into the picture as it solves both problems: it scales linearly with the number of observations and performs well on very sparse and imbalanced datasets.</p> <h3 id="pmf-algorithm">PMF algorithm</h3> <p>Suppose we have \(M\) movies, \(N\) users, and \(R_{ij}\) represent the rating of user \(i\) for movie \(j\). (for the sake of simplicity it’s assumed rating values are integer \(\in [1, K]\))<br/></p> <div align="center"> $$U \in R^{D\times N}$$ (latent user matrix)<br/> $$V \in R^{DxM}$$ (latent movie matrix) </div> <p><br/> For test set, assume a probabilistic linear model (relationship between input and output is linear but observed outputs deviate from this linear relationship due to random noise). Define conditional distribution over the observed ratings as: <br/></p> <div align="center"> $$ p(R|U, V, \sigma ^2)=\prod_{i = 1}^{N}\prod_{j = 1}^{M}\left[ N(R_{ij}|U^T_iV_j, \sigma ^2)\right]^{I_{ij}} $$ </div> <p>where, <br/> \(\hspace{20pt} N(x|\mu, \sigma ^2)\) denotes probability density function of Gaussian distribution with mean \(\mu\) and variance \(\sigma ^2\) <br/> \(\hspace{20pt}I_{ij}=1\), if user \(i\) rated movie \(j\)<br/> \(\hspace{33pt}=0\), otherwise <br/> <br/> We also place zero-mean spherical Gussian priors on user and movie feature vectors. <br/> <br/> \(\hspace{60pt} p(U|\sigma ^2_{U})=\prod_{i=1}^{N} N(U_i|0, \sigma ^2_{U}I), \quad p(V|\sigma ^2_{V})=\prod_{i=1}^{N} N(V_i|0, \sigma ^2_{V}I)\) <br/> <br/> Using Bayes’ Rule, we find that the posterior distribution over latent vectors \(U_i\) and \(V_i\) given the observed ratings \(R\) is proportional to the product of the likelihood of the observed data and the prior distribution. Taking log of the posterior distribution over the user and movie features, we get:<br/> <br/> \(\hspace{10pt} ln \, p(U, V|R, \sigma ^2, \sigma ^2_V, \sigma ^2_U) = - \frac{1}{2\sigma ^2}\sum_{i = 1}^{N}\sum_{j = 1}^{M}I_{ij}(R_{ij}-U_i^TV_j)^2 - \frac{1}{2\sigma ^2_U}\sum_{i = 1}^{N}U_i^TU_i - \frac{1}{2\sigma ^2_V}\sum_{j = 1}^{M}V_j^TV_j\) \(\hspace{125pt} - \frac{1}{2}\left( \left( \sum_{i = 1}^{N}\sum_{j = 1}^{M}I_{ij}\right )ln\, \sigma ^2 + ND ln\, \sigma ^2_U + MD ln \,\sigma ^2_V \right)\) <br/> <br/> Maximising the log-posterior over movie and user features with hyperparameters (observation noise variance \(\sigma\) and prior variances \(\sigma_U, \sigma_V\)) kept fixed is equivalent to minimizing the sum-of-squared-errors obective function with quadratic regularization terms: <br/> <br/> \(\hspace{40pt} E = \frac{1}{2}\sum_{i = 1}^{N}\sum_{j = 1}^{M}I_{ij}(R_{ij}-U_i^TV_j)^2 + \frac{\lambda _U}{2}\sum_{i = 1}^{N} ||U_i||^2_{Fro} + \frac{\lambda _V}{2}\sum_{j = 1}^{M} ||V_i||^2_{Fro}\) <br/> <br/> \(\hspace{150pt}\) where, \(\lambda _U=\frac{\sigma ^2}{\sigma ^2_U}, \lambda _V=\frac{\sigma ^2}{\sigma ^2_V}\) <br/> <br/> A local minimum of this objective function can be found by performing gradient descent in \(U\) and \(V\). <br/></p> <blockquote> <p>If all the ratings have been observed it means the estimates of the parameters (eg. user and latent features) can be made with higher confidence and potentially less variance in the estimates themselves.</p> </blockquote> <p>PMF model extends traditional SVD by incorporating probabilistic elements (gaussian noise in observed ratings, gaussian priors for latent features). This probabilistic framework accounts for uncertainty in observation (ratings) and parameter estimates (latent features) offering a more flexible and potentially robust approach compared to deterministic SVD.</p> <p>In a typical real-world scenario, not all user-item ratings are available. However, if hypothetically, all ratings were observed, the PMF model would still need to balance fitting the observed data (through the likelihood term) and maintaining reasonable complexity (through the prior regularization terms).</p> <p>If prior variances are made very large, then it effectively weakens the influence of the prior distribution, making it non-informative. This means the latent features are less regularized and the model focuses more on fitting the observed data. In the limit, when the prior variances go to infinity, the regularization effect vanishes, and the PMF model then essentially reduces to performing an SVD.</p> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1"> <p>R. Salakhutdino and A. Mnihv, Probabilistic Matrix Factorization <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><summary type="html"><![CDATA[Scalable solution for large, sparse, and imbalanced data]]></summary></entry><entry><title type="html">Variational Calculus</title><link href="https://gshivansh2001.github.io/blog/2023/vc/" rel="alternate" type="text/html" title="Variational Calculus"/><published>2023-11-12T00:00:00+00:00</published><updated>2023-11-12T00:00:00+00:00</updated><id>https://gshivansh2001.github.io/blog/2023/vc</id><content type="html" xml:base="https://gshivansh2001.github.io/blog/2023/vc/"><![CDATA[<h3 id="introduction">Introduction</h3> <p>Variational calculus is a field of mathematics which is based on the idea of using variation to optimize functionals.</p> <h4 id="functionals-theory">Functionals Theory</h4> <p>A functional is defined by a rule, which associates a number (real or complex) with a function of one or several variables,</p> \[f(r1,...) \xrightarrow{rule} F[ f ],\] <p>or, more generally, which associates a number with a set of functions,</p> \[f1, f2,... \xrightarrow{rule} F[ f1, f2,...]\] <p>In such a case, functional can be defined as “function of a function”. An example for a functional is a definite integral over a continuous function.</p> <h4 id="calculus-of-variations">Calculus of Variations</h4> <p>This field is concerned with the maxima or minima of functionals. The extrema of functionals may be obtained by finding functions for which the functional derivative is equal to zero.</p> <blockquote> <p>A very common problem is the problem of finding the curve of shortest length between two given points. With no constraints, the answer is simple: a straight line. But in case of constraints on curve, the solution is not that straightforward. Geodesics represent these solutions.</p> </blockquote> <p>Consider a functional \(J[y(x)]\) of \(y(x)\),</p> \[J[y] = \int_{x_1}^{x_2} L(x, y(x), y'(x), ..., y^{(n)}(x)) dx\] <p>where \(L\) is the integrand of the functional. Now, <em>variation</em> of the functional is the amount the functional changes when the input function is changed by a little bit. Suppose we let \(y(x) \xrightarrow{} y(x) + \delta y(x)\), then</p> \[y^{(n)}(x) \xrightarrow{} y^{(n)}(x) + \frac{\mathrm{d^n}}{\mathrm{d}x^n}\delta y(x) = y^{(n)}(x) + \delta y^{(n)}(x)\] <p>(assuming that the integrand \(L\) is continuously differentiable) Now,</p> \[J[y + \delta y] = \int_{x_1}^{x_2} L(x, y + \delta y, y' + \delta y', ..., y^{(n)} + \delta y^{(n)}) dx\] <p>Using first-order Taylor expansion for a multivariable function, we get:</p> \[J[y + \delta y] = \int_{x_1}^{x_2} \Biggl( L + \frac{\partial L}{\partial x}dx + \frac{\partial L}{\partial y}\delta y + \frac{\partial L}{\partial y'}\delta y' + ... + \frac{\partial^{(n)} L}{\partial y^{(n)}}\delta y^{(n)} \Biggr) dx\] <p>To find the variation for this functional, we calculate:</p> \[\delta J = J[y + \delta y] - J[y]\] \[\qquad \qquad = \int_{x_1}^{x_2} \Biggl( \frac{\partial L}{\partial x}dx + \frac{\partial L}{\partial y}\delta y + \frac{\partial L}{\partial y'}\delta y' + ... + \frac{\partial^{(n)} L}{\partial y^{(n)}}\delta y^{(n)} \Biggr) dx\] <p>Applying integration by parts repeatedly we get,</p> \[\begin{align*} \delta J = \frac{d}{dx}(\frac{\partial L}{\partial y'})\delta y(x)|_{x_1}^{x_2} + \frac{d}{dx}(\frac{\partial L}{\partial y''})\delta y'(x)|_{x_1}^{x_2} - \frac{d^2}{dx^2}(\frac{\partial L}{\partial y''})\delta y(x)|_{x_1}^{x_2} \\ + ... + (-1)^{n-1}\frac{d^n}{dx^n}(\frac{\partial L}{\partial y^{(n)}})\delta y^{(n)}(x)|_{x_1}^{x_2} + \int_{x_1}^{x_2} \frac{\partial L}{\partial x} dx + \\ \int_{x_1}^{x_2} \Biggl( \frac{\partial L}{\partial y} - \frac{d}{dx}\frac{\partial L}{\partial y'} + \frac{d^2}{dx^2}\frac{\partial L}{\partial y''} + ... + (-1)^{n-1}\frac{d^n}{dx^n}\frac{\partial L}{\partial y^{(n)}} \Biggr)\delta y(x)dx \end{align*}\] <p>The above is the variation of the functional \(J\). One of the practical examples is the problem of lower bounding the marginal likelihood using variation.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[A brief introduction]]></summary></entry><entry><title type="html">Rényi DP</title><link href="https://gshivansh2001.github.io/blog/2023/rdp/" rel="alternate" type="text/html" title="Rényi DP"/><published>2023-04-26T00:00:00+00:00</published><updated>2023-04-26T00:00:00+00:00</updated><id>https://gshivansh2001.github.io/blog/2023/rdp</id><content type="html" xml:base="https://gshivansh2001.github.io/blog/2023/rdp/"><![CDATA[<h4 id="rényi-divergence">Rényi Divergence</h4> <p>Before we go into Rényi DP, we need to understand what Rényi divergence is. Rényi divergence is a way of measuring the matching between two distributions. It’s formally defined as follows:</p> \[D_\alpha(P||Q) = \frac{1}{\alpha - 1}log(\sum_{i=1}^{n}\frac{p_{i}^{\alpha}}{q_{i}^{\alpha - 1}})\] <p>when \(0&lt; \alpha &lt; \infty\) and \(\alpha \neq 1\). We can define the Rényi divergence for the special values \(\alpha = 0, 1, \infty\) by taking a limit.</p> <h4 id="formal-definition-of-rényi-dp">Formal definition of Rényi DP</h4> <p>Rényi differential privacy (RDP)<sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup> is a generalization of ε-differential privacy. A randomized mechanism \(M\) is said to have: Rényi DP of order \(\alpha\), or \((\alpha,\epsilon)\)-RDP, if for any neighbouring databases \(x\) and \(x'\), it holds that,</p> \[D_\alpha(M(x)||M(x')) \le \epsilon\] <p>A mechanism satisfying ε-DP is equivalent to saying that it satisfies RDP of order \(\infty\). RDP gives privacy guarantees that are somewhere between ε-DP and \((\epsilon, \delta)\) -DP.</p> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1"> <p>Ilya Mironov. Rényi differential privacy. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><summary type="html"><![CDATA[A natural relaxation of DP]]></summary></entry></feed>