<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://gshivansh2001.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://gshivansh2001.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-11-26T20:52:58+00:00</updated><id>https://gshivansh2001.github.io/feed.xml</id><title type="html">Shivansh Gupta</title><subtitle>Shivansh Gupta&apos;s Academic Webpage</subtitle><entry><title type="html">Proabilistic Matrix Factorization</title><link href="https://gshivansh2001.github.io/blog/2024/pmf/" rel="alternate" type="text/html" title="Proabilistic Matrix Factorization"/><published>2024-05-02T00:00:00+00:00</published><updated>2024-05-02T00:00:00+00:00</updated><id>https://gshivansh2001.github.io/blog/2024/pmf</id><content type="html" xml:base="https://gshivansh2001.github.io/blog/2024/pmf/"><![CDATA[<p>This paper<sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup> presents the Probabilistic Matrix Factorization (PMF) model which scales linearly with the number of observations and performs well on the large, sparse, and imbalanced dataset. Before going into the detail of this model, let’s discuss why it is even needed?</p> <p>Recommendation systems enhance user experiences by suggesting relevant items based on preferences. They primarily use two traditional methods: collaborative filtering, which recommends items liked by other users with similar tastes and content-based filtering, which recommends items similar to those a user has liked before. The most popular approach to collaborative filtering is based on low-dimensional factor models.</p> <p>Many of these low-dimensional models are probabilistic factor-based models. The majority of these probabilistic factor-based models attempt to explain observations through underlying factors or variables (hidden factors; these represent latent characteristics). The problem with these models is that exact inference is intractable, meaning that determining the distribution of hidden factors given the observed data is computationally impractical. This necessitates the use of approximations to estimate the posterior distribution of hidden factors based on observed data. Singular Value Decomposition (SVD) is one of the low-rank approximation algorithms based on minimizing the sum-squared distance. It finds the matrix \(\hat R=U^TV\) of the given rank which minimizes the sum-squared distance to the target matrix \(R\). Collaborative filtering algorithms need to scale linearly with number of observations and perform well on very sparse and imbalanced datasets. Most of the aforementioned methods prove infeasible for various reasons. Firstly, except for the matrix-factorization based methods, none scale well to large datasets. Secondly, if most of the rating entries are missing, it leads to inaccurate predictions. Here, PMF comes into the picture as it solves both problems: it scales linearly with the number of observations and performs well on very sparse and imbalanced datasets.</p> <h3 id="pmf-algorithm">PMF algorithm</h3> <p>Suppose we have \(M\) movies, \(N\) users, and \(R_{ij}\) represent the rating of user \(i\) for movie \(j\). (for the sake of simplicity it’s assumed rating values are integer \(\in [1, K]\))<br/></p> <div align="center"> $$U \in R^{D\times N}$$ (latent user matrix)<br/> $$V \in R^{DxM}$$ (latent movie matrix) </div> <p><br/> For test set, assume a probabilistic linear model (relationship between input and output is linear but observed outputs deviate from this linear relationship due to random noise). Define conditional distribution over the observed ratings as: <br/></p> <div align="center"> $$ p(R|U, V, \sigma ^2)=\prod_{i = 1}^{N}\prod_{j = 1}^{M}\left[ N(R_{ij}|U^T_iV_j, \sigma ^2)\right]^{I_{ij}} $$ </div> <p>where, <br/> \(\hspace{20pt} N(x|\mu, \sigma ^2)\) denotes probability density function of Gaussian distribution with mean \(\mu\) and variance \(\sigma ^2\) <br/> \(\hspace{20pt}I_{ij}=1\), if user \(i\) rated movie \(j\)<br/> \(\hspace{33pt}=0\), otherwise <br/> <br/> We also place zero-mean spherical Gussian priors on user and movie feature vectors. <br/> <br/> \(\hspace{60pt} p(U|\sigma ^2_{U})=\prod_{i=1}^{N} N(U_i|0, \sigma ^2_{U}I), \quad p(V|\sigma ^2_{V})=\prod_{i=1}^{N} N(V_i|0, \sigma ^2_{V}I)\) <br/> <br/> Using Bayes’ Rule, we find that the posterior distribution over latent vectors \(U_i\) and \(V_i\) given the observed ratings \(R\) is proportional to the product of the likelihood of the observed data and the prior distribution. Taking log of the posterior distribution over the user and movie features, we get:<br/> <br/> \(\hspace{10pt} ln \, p(U, V|R, \sigma ^2, \sigma ^2_V, \sigma ^2_U) = - \frac{1}{2\sigma ^2}\sum_{i = 1}^{N}\sum_{j = 1}^{M}I_{ij}(R_{ij}-U_i^TV_j)^2 - \frac{1}{2\sigma ^2_U}\sum_{i = 1}^{N}U_i^TU_i - \frac{1}{2\sigma ^2_V}\sum_{j = 1}^{M}V_j^TV_j\) \(\hspace{125pt} - \frac{1}{2}\left( \left( \sum_{i = 1}^{N}\sum_{j = 1}^{M}I_{ij}\right )ln\, \sigma ^2 + ND ln\, \sigma ^2_U + MD ln \,\sigma ^2_V \right)\) <br/> <br/> Maximising the log-posterior over movie and user features with hyperparameters (observation noise variance \(\sigma\) and prior variances \(\sigma_U, \sigma_V\)) kept fixed is equivalent to minimizing the sum-of-squared-errors obective function with quadratic regularization terms: <br/> <br/> \(\hspace{40pt} E = \frac{1}{2}\sum_{i = 1}^{N}\sum_{j = 1}^{M}I_{ij}(R_{ij}-U_i^TV_j)^2 + \frac{\lambda _U}{2}\sum_{i = 1}^{N} ||U_i||^2_{Fro} + \frac{\lambda _V}{2}\sum_{j = 1}^{M} ||V_i||^2_{Fro}\) <br/> <br/> \(\hspace{150pt}\) where, \(\lambda _U=\frac{\sigma ^2}{\sigma ^2_U}, \lambda _V=\frac{\sigma ^2}{\sigma ^2_V}\) <br/> <br/> A local minimum of this objective function can be found by performing gradient descent in \(U\) and \(V\). <br/></p> <blockquote> <p>If all the ratings have been observed it means the estimates of the parameters (eg. user and latent features) can be made with higher confidence and potentially less variance in the estimates themselves.</p> </blockquote> <p>PMF model extends traditional SVD by incorporating probabilistic elements (gaussian noise in observed ratings, gaussian priors for latent features). This probabilistic framework accounts for uncertainty in observation (ratings) and parameter estimates (latent features) offering a more flexible and potentially robust approach compared to deterministic SVD.</p> <p>In a typical real-world scenario, not all user-item ratings are available. However, if hypothetically, all ratings were observed, the PMF model would still need to balance fitting the observed data (through the likelihood term) and maintaining reasonable complexity (through the prior regularization terms).</p> <p>If prior variances are made very large, then it effectively weakens the influence of the prior distribution, making it non-informative. This means the latent features are less regularized and the model focuses more on fitting the observed data. In the limit, when the prior variances go to infinity, the regularization effect vanishes, and the PMF model then essentially reduces to performing an SVD.</p> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1"> <p>R. Salakhutdino and A. Mnihv, Probabilistic Matrix Factorization <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><summary type="html"><![CDATA[Scalable solution for large, sparse, and imbalanced data]]></summary></entry><entry><title type="html">Variational Calculus</title><link href="https://gshivansh2001.github.io/blog/2023/vc/" rel="alternate" type="text/html" title="Variational Calculus"/><published>2023-11-12T00:00:00+00:00</published><updated>2023-11-12T00:00:00+00:00</updated><id>https://gshivansh2001.github.io/blog/2023/vc</id><content type="html" xml:base="https://gshivansh2001.github.io/blog/2023/vc/"><![CDATA[<h3 id="introduction">Introduction</h3> <p>Variational calculus is a field of mathematics which is based on the idea of using variation to optimize functionals.</p> <h4 id="functionals-theory">Functionals Theory</h4> <p>A functional is defined by a rule, which associates a number (real or complex) with a function of one or several variables,</p> \[f(r1,...) \xrightarrow{rule} F[ f ],\] <p>or, more generally, which associates a number with a set of functions,</p> \[f1, f2,... \xrightarrow{rule} F[ f1, f2,...]\] <p>In such a case, functional can be defined as “function of a function”. An example for a functional is a definite integral over a continuous function.</p> <h4 id="calculus-of-variations">Calculus of Variations</h4> <p>This field is concerned with the maxima or minima of functionals. The extrema of functionals may be obtained by finding functions for which the functional derivative is equal to zero.</p> <blockquote> <p>A very common problem is the problem of finding the curve of shortest length between two given points. With no constraints, the answer is simple: a straight line. But in case of constraints on curve, the solution is not that straightforward. Geodesics represent these solutions.</p> </blockquote> <p>Consider a functional \(J[y(x)]\) of \(y(x)\),</p> \[J[y] = \int_{x_1}^{x_2} L(x, y(x), y'(x), ..., y^{(n)}(x)) dx\] <p>where \(L\) is the integrand of the functional. Now, <em>variation</em> of the functional is the amount the functional changes when the input function is changed by a little bit. Suppose we let \(y(x) \xrightarrow{} y(x) + \delta y(x)\), then</p> \[y^{(n)}(x) \xrightarrow{} y^{(n)}(x) + \frac{\mathrm{d^n}}{\mathrm{d}x^n}\delta y(x) = y^{(n)}(x) + \delta y^{(n)}(x)\] <p>(assuming that the integrand \(L\) is continuously differentiable) Now,</p> \[J[y + \delta y] = \int_{x_1}^{x_2} L(x, y + \delta y, y' + \delta y', ..., y^{(n)} + \delta y^{(n)}) dx\] <p>Using first-order Taylor expansion for a multivariable function, we get:</p> \[J[y + \delta y] = \int_{x_1}^{x_2} \Biggl( L + \frac{\partial L}{\partial x}dx + \frac{\partial L}{\partial y}\delta y + \frac{\partial L}{\partial y'}\delta y' + ... + \frac{\partial^{(n)} L}{\partial y^{(n)}}\delta y^{(n)} \Biggr) dx\] <p>To find the variation for this functional, we calculate:</p> \[\delta J = J[y + \delta y] - J[y]\] \[\qquad \qquad = \int_{x_1}^{x_2} \Biggl( \frac{\partial L}{\partial x}dx + \frac{\partial L}{\partial y}\delta y + \frac{\partial L}{\partial y'}\delta y' + ... + \frac{\partial^{(n)} L}{\partial y^{(n)}}\delta y^{(n)} \Biggr) dx\] <p>Applying integration by parts repeatedly we get,</p> \[\begin{align*} \delta J = \frac{d}{dx}(\frac{\partial L}{\partial y'})\delta y(x)|_{x_1}^{x_2} + \frac{d}{dx}(\frac{\partial L}{\partial y''})\delta y'(x)|_{x_1}^{x_2} - \frac{d^2}{dx^2}(\frac{\partial L}{\partial y''})\delta y(x)|_{x_1}^{x_2} \\ + ... + (-1)^{n-1}\frac{d^n}{dx^n}(\frac{\partial L}{\partial y^{(n)}})\delta y^{(n)}(x)|_{x_1}^{x_2} + \int_{x_1}^{x_2} \frac{\partial L}{\partial x} dx + \\ \int_{x_1}^{x_2} \Biggl( \frac{\partial L}{\partial y} - \frac{d}{dx}\frac{\partial L}{\partial y'} + \frac{d^2}{dx^2}\frac{\partial L}{\partial y''} + ... + (-1)^{n-1}\frac{d^n}{dx^n}\frac{\partial L}{\partial y^{(n)}} \Biggr)\delta y(x)dx \end{align*}\] <p>The above is the variation of the functional \(J\). One of the practical examples is the problem of lower bounding the marginal likelihood using variation.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[A brief introduction]]></summary></entry><entry><title type="html">Rényi DP</title><link href="https://gshivansh2001.github.io/blog/2023/rdp/" rel="alternate" type="text/html" title="Rényi DP"/><published>2023-04-26T00:00:00+00:00</published><updated>2023-04-26T00:00:00+00:00</updated><id>https://gshivansh2001.github.io/blog/2023/rdp</id><content type="html" xml:base="https://gshivansh2001.github.io/blog/2023/rdp/"><![CDATA[<h4 id="rényi-divergence">Rényi Divergence</h4> <p>Before we go into Rényi DP, we need to understand what Rényi divergence is. Rényi divergence is a way of measuring the matching between two distributions. It’s formally defined as follows:</p> \[D_\alpha(P||Q) = \frac{1}{\alpha - 1}log(\sum_{i=1}^{n}\frac{p_{i}^{\alpha}}{q_{i}^{\alpha - 1}})\] <p>when \(0&lt; \alpha &lt; \infty\) and \(\alpha \neq 1\). We can define the Rényi divergence for the special values \(\alpha = 0, 1, \infty\) by taking a limit.</p> <h4 id="formal-definition-of-rényi-dp">Formal definition of Rényi DP</h4> <p>Rényi differential privacy (RDP)<sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup> is a generalization of ε-differential privacy. A randomized mechanism \(M\) is said to have: Rényi DP of order \(\alpha\), or \((\alpha,\epsilon)\)-RDP, if for any neighbouring databases \(x\) and \(x'\), it holds that,</p> \[D_\alpha(M(x)||M(x')) \le \epsilon\] <p>A mechanism satisfying ε-DP is equivalent to saying that it satisfies RDP of order \(\infty\). RDP gives privacy guarantees that are somewhere between ε-DP and \((\epsilon, \delta)\) -DP.</p> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1"> <p>Ilya Mironov. Rényi differential privacy. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><summary type="html"><![CDATA[A natural relaxation of DP]]></summary></entry><entry><title type="html">Differential Privacy</title><link href="https://gshivansh2001.github.io/blog/2023/dp/" rel="alternate" type="text/html" title="Differential Privacy"/><published>2023-04-25T00:00:00+00:00</published><updated>2023-04-25T00:00:00+00:00</updated><id>https://gshivansh2001.github.io/blog/2023/dp</id><content type="html" xml:base="https://gshivansh2001.github.io/blog/2023/dp/"><![CDATA[<h4 id="why-other-approaches-to-privacy-not-enough">Why other approaches to privacy not enough?</h4> <p>Most of the approaches used for privacy protection are susceptible to certain attacks. For example, the Homogeneity attack is a known attack on k-Anonymity, allowing adversaries to obtain the exact values of sensitive attributes without re-identifying them from released data. All these approaches to privacy do not provide the privacy guarantee. But DP provides robust privacy guarantees against a wide range of privacy attacks. It makes sure that the data subject is not affected by their entry or participation in a database, while maximizing utility/data accuracy (as opposed to random/empty outputs) for queries to a database. It guarantees that the impact on the participant of a study is the same, regardless of whether they were in the study or not. It is the conclusions reached in the study that affect the participant, not his presence or absence in the data set.</p> <h4 id="how-dp-protects-privacy">How DP protects privacy?</h4> <p>Differential Privacy is a method of protecting individuals’ privacy when their data is collected and analyzed. When managing a sensitive database, it’s essential to ensure that adversaries cannot learn confidential or sensitive information from the statistics of the data. Differential Privacy makes it harder for an adversary to breach privacy. It adds noise to the ground truth of queries to a database. DP makes sure that the output of two neighbouring databases after querying the database is pretty much the same. Two databases, \(x\) and \(x'\) are called neighbouring databases if they differ in the data of a single individual. DP makes sure that the observed output of a query does not reveal which of \(x\) or \(x'\) was the input database.</p> <h4 id="formal-definition-of-differential-privacy">Formal definition of Differential Privacy</h4> <p>We say that a mechanism \(M\) satisfies differential privacy if for all neighbouring databases \(x\) and \(x'\), and all possible outputs \(S\),</p> \[P[M(x)=S] \le e^\epsilon P[M(x')=S]\] <blockquote> <p>must be true for all possible outputs \(S\)</p> </blockquote> <p>The mechanism is \(\epsilon\)-differentially private in such a case. For any pair of databases, we cannot gain more than a small amount of probabilistic information about a single individual. When adding or removing an individual in the database, the output distribution changes, but this ensures that the difference between the probabilities of any two outputs is bounded by a factor \(\epsilon\). \(\epsilon\) in the above definition is called privacy budget and \(e^\epsilon\) is called privacy parameter or the privacy loss parameter.</p> <p><strong>How is \(\epsilon\) related to levels of privacy?</strong> If \(\epsilon\) is small, the mechanism provides very similar outputs when given similar inputs which implies higher levels of privacy. But if \(\epsilon\) is large, the mechanism provides less similar outputs and therefore, lesser privacy.</p> <h4 id="how-to-achieve-dp">How to achieve DP?</h4> <p>Easiest way to achieve DP for a query is to add random noise to its answer. It can be obtained by</p> <ol> <li>Computing a function \(F\) of the data,</li> <li>Adding some noise to the result value.</li> </ol> <p>The noise must be large enough to hide an individual contribution so as to satisfy the definition of differential privacy but not so much that the result becomes too noisy to be useful. But for some functions F, an individual contribution can change the true result more than for other functions. This concept is captured by sensitivity: the higher the possible change, the higher the sensitivity. The sensitivity of a function measures the degree to which a single individual’s data can change the function in the worst case. And typically, to achieve \(\epsilon\)-differential privacy with a fixed \(\epsilon\), you have to add more noise when the sensitivity of the function is higher (to compensate).</p> <p>For a function \(f:D\rightarrow R^k\), sensitivity of \(f\) is:</p> \[\Delta f=\underset{D_1,D_2}{\max}||f(D_1)-f(D_2)||_1\] <p>where datasets \(D_1\) and \(D_2\) are neighbouring datasets. This is called \(l_1\)- sensitivity of function \(f\). The \(l_1\)- sensitivity gives an upper bound on how much we need to perturb the data to preserve privacy. This discussion brings query sensitivity into the picture. Let’s talk about that.</p> <h4 id="query-sensitivity">Query Sensitivity</h4> <p>Sensitivity of a query function is defined as the maximum absolute difference between query results of the neighbouring databases over all possible neighbouring databases, for a given input dataset. There are different types of query, each with its own sensitivity. One type of query is the count query. Count query counts the number of rows in the dataset which satisfy the property specified in the query. For example, a count query might count the number of rows in a dataset where a certain attribute equals a specific value. The \(l_1\)- sensitivity of counting queries is 1 as adding a row to the dataset can increase the output of the query by at most 1: either the new row has the desired property, and the count increases by 1, or it does not, and the count stays the same (the count may correspondingly decrease when a row is removed). Therefore, the sensitivity of the count query is 1. In contrast, the sensitivity of a sum query depends on the contents of the dataset. Sum query sums up the attribute values of dataset rows. In this case, adding a new row to the dataset will increase the query result by at most the maximum attribute value across all rows, which could be any value and is not necessarily 1 as in the count query.</p> <h4 id="epsilon-delta-dp">\((\epsilon, \delta)\)-DP</h4> <p>A mechanism \(M\) satisfies \((\epsilon, \delta)\)- differential privacy if for all neighbouring databases \(x\) and \(x'\), and all possible outputs \(S\),</p> \[P[M(x)=S] \le e^\epsilon P[M(x')=S] +\delta\] <p>\((\epsilon, \delta)\)- DP ensures that for all neighbouring databases, the absolute value of privacy loss will be bounded by \(\epsilon\) with probability at least \(1-\delta\). \(\epsilon\) is independent of the size of the database, whereas in case of \(\delta\), the chances of privacy leak might increase with the size of the database. Hence, ideally, we would want to set the \(\delta\) value to be less than the inverse of the size of the database. However, it is worth noting that \((\epsilon, \delta)\)-DP may not always succeed, as there is a possibility of output that occurs with a non-zero probability \(\delta\) when the individual’s data is present, but never happens otherwise.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[What is it and why is it needed]]></summary></entry></feed>